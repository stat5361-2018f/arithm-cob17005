---
title: "Homework 2 in R Markdown"
# subtitle: "possible subtitle goes here"
author:
  - Cosmin Borsa^[<cosmin.borsa@uconn.edu>; M.S. in Applied Financial Mathematics,
    Department of Mathematics, University of Connecticut.]
date: "`r format(Sys.time(), '%d %B %Y')`"
documentclass: article
papersize: letter
fontsize: 11pt
bibliography: template.bib
biblio-style: asa
keywords: R Markdown, bookdown, Normal Distibution, Monte-Carlo methods, Computer Arithmetics
# keywords set in YAML header here only go to the properties of the PDF output
# the keywords that appear in PDF output are set in latex/before_body.tex
output:
  bookdown::pdf_document2
  bookdown::html_document2
abstract: |
    In this document we will approximate the probability distribution function of a standard normal distribution using Monte-Carlo methods. We will also talk about some topics related to the double-precision binary floating-point format.
---


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
## some utility functions, see the source code for details
source("utils_template.R")

## specify the packages needed
pkgs <- c("splines2", "DT", "webshot", "leaflet", "graphics")
need.packages(pkgs)

## external data can be read in by regular functions,
## such as read.table or load

## for latex and html output
isHtml <- knitr::is_html_output()
isLatex <- knitr::is_latex_output()
latex <- ifelse(isLatex, '\\LaTeX\\', 'LaTeX')

## specify global chunk options
knitr::opts_chunk$set(fig.width = 5, fig.height = 4, dpi = 300,
                      out.width = "90%", fig.align = "center")

```


# Introduction {#sec:intro}

The standard normal distribution is probably the most popular continuous probability distribution. It is used not only in statistics, but also in the natural and social sciences. The probability density function of the standard normal distribution is given by the following formula:

\begin{align}
    \Phi(t) = \int_{-\infty}^{t} \dfrac{1}{\sqrt{2\pi}}e^{\frac{-y^2}{2}}dy.
    (\#eq:normal)
\end{align}

The probability density function of the standard normal distribution $\Phi(t)$ cannot be expressed in terms of elementary functions; thus, it doesn't have a closed-form expression. In evaluate the probability density function $\Phi(t)$, we will use Monte Carlo simulations. 

To estimate the probability density function $\Phi(t)$, we are going to first generate $n$ normally distributed random observations $X_i$. Then, we are going to count how many of them are less than  a given value $t$. To keep things fairly straight forward, we are going to have $n \in \{100, 1000, 10000\}$ and $t \in \{0.0, 0.67, 0.84, 1.28, 1.65, 2.32, 2.58, 3.09, 3.72\}$. The Monte-Carlo estimate of the probability density function is going to be given by the formula \@ref(eq:estimation) and is represented by $\hat{\Phi(t)}$.

\begin{align}
    \hat{\Phi}(n,t) = \dfrac{1}{n} \sum_{i=1}^{n} I(X_{i} \leq t).
    (\#eq:estimation)
\end{align}

# Monte Carlo Implementation {#sec:implementation}

For implementation of the Monte-Carlo method into R, we are going to define a function `MC_normal_dist`. This function will estimate the probability density function of the standard normal distribution $\Phi(t)$ using the formula \@ref(eq:estimation).

```{r normal dist, echo = TRUE, message = FALSE, warning = FALSE}
MC_normal_dist<-function (n,t) 
{
inc<-0
randv<-rnorm(n)
for(i in 1:n){
  if(randv[i] <= t){inc = inc + 1}
  }
estprob <- inc/n
return(estprob)
}
```

Next, we are going to use the function `MC_normal_dist` to create a table with the various values of $n$ and $t$. The function `MC_table` does this task. It saves the values of $t$ in a vector and uses a for loop to compute the estimates for each $n$. In order to display the table in an orderly fashion, we have rounded the results of the estimation to 3 decimal places. We have also used the `set.seed(1)` function in order to fix the random generated numbers.

```{r table, echo = TRUE, message = FALSE, warning = FALSE}

MC_table<-function () 
{
  set.seed(1)
  
  t <- c(0.0,0.67,0.84,1.28,1.65,2.32,2.58,3.09,3.72)
  t_column <-numeric(length(t))
  est100 <-numeric(length(t))
  est1000 <-numeric(length(t))
  est10000 <-numeric(length(t))
  true_val <-numeric(length(t))
  
  for (i in 1:length(t)){
    t_column[i] <- t[i]
    est100[i] <- signif(MC_normal_dist(100,t[i]), digits = 3)
    est1000[i] <- signif(MC_normal_dist(1000,t[i]), digits = 3)
    est10000[i] <- signif(MC_normal_dist(10000,t[i]), digits = 3)
    true_val[i] <- signif(pnorm(t[i]), digits = 3)
  }
  
  matr <-cbind(t_column,true_val, est100,est1000,est10000)
  colnames(matr) <- c("t", "True Value", "n = 100", "n = 1,000", "n = 10,000")

    return(matr)
}
```

Next we would like to generate the table and display it using the function `knitr::kable`.

(ref:implementation) Comparison of Monte-Carlo estimates and the true values of the probability density function.

```{r estimation, echo = FALSE}
knitr::kable(MC_table(), caption = '(ref:implementation)')
```

# Box Plots {#sec:BoxPlots}

In this section we are going to evaluate how well the Monte-Carlo method has estimated the the probability density function $\Phi(t)$ at various levels of $t$. For this task we have computed the approximation error $\epsilon(t)$ by substracting the true value of the probability density function from each Monte-Carlo estimation.
$$\epsilon(t) = \hat\Phi(t) - \Phi(t)$$ 
In order to vizualize the errors at each level of $t$, we have generated 100 approximation errors for each pair of $n$ and $t$. The results are displayed using `ggplot2` in the following 3 figures.

```{r boxplot, echo = FALSE}
set.seed(1)
library(ggplot2)
t <- c(0.0,0.67,0.84,1.28,1.65,2.32,2.58,3.09,3.72)
true_val <-numeric(length(t))

for (i in 1:length(t)){
    true_val[i] <- signif(pnorm(t[i]), digits = 4)
}
err100 <- matrix(0, nrow = length(t), ncol = 100)
err1000 <- matrix(0, nrow = length(t), ncol = 100)
err10000 <- matrix(0, nrow = length(t), ncol = 100)
for (i in 1:length(t)){
  for (j in 1:100){
    err100[i, j] <- MC_normal_dist(100, t[i]) - true_val[i]
    err1000[i, j] <- MC_normal_dist(1000, t[i]) - true_val[i]
    err10000[i, j] <- MC_normal_dist(1000, t[i]) - true_val[i]
  }
  
}

ggplot(stack(data.frame(t(err100))), aes(x=ind, y=values))+
  geom_boxplot()+
  scale_x_discrete(labels = t, name = "t")+
  scale_y_continuous(name = "Error scale")+
  ggtitle("Approximation error for n = 100")+
  theme(plot.title = element_text(size = 10, hjust = 0.3))

ggplot(stack(data.frame(t(err1000))), aes(x=ind, y=values))+
  geom_boxplot()+
  scale_x_discrete(labels = t, name = "t")+
  scale_y_continuous(name="Error scale")+
  ggtitle("Approximation error for n = 1,000")+
  theme(plot.title = element_text(size = 10, hjust = 0.3))

ggplot(stack(data.frame(t(err10000))), aes(x=ind, y=values))+
  geom_boxplot()+
  scale_x_discrete(labels = t, name = "t")+
  scale_y_continuous(name = "Error scale")+
  ggtitle("Approximation error for n = 10,000")+
  theme(plot.title = element_text(size = 10, hjust = 0.3))

```

# Floating-point arithmetic {#sec:Floatingpoint}

In this section we are going to explain how `.Machine$double.xmax`, `.Machine$double.xmin`, `.Machine$double.eps`, and `.Machine@double.neg.eps` are defined using the 64-bit double precision floating point arithmetic. First of all, `Machine()` is a function that returns information on the numeric characteristics of the computer that R is running on. Such characteristics include the largest double or the machine's precision. `.Machine` is the variable that stores these information.

`.Machine$double.xmax` gives the largest finite floating-point number. In the double-precision binary floating-point format this number is represented by: $$0\ \ 11111111110\ \ 1111111111111111111111111111111111111111111111111111$$
On this computer the value of `.Machine$double.xmax` is:

```{r, echo = FALSE}
options(digits = 20)
.Machine$double.xmax
```

`.Machine$double.xmin` gives the smallest non-zero normalized floating-point number. In the double-precision binary floating-point format this number is represented by: $$0\ \ 00000000001\ \ 0000000000000000000000000000000000000000000000000000$$
On this computer the value of `.Machine$double.xmin` is:

```{r, echo = FALSE}
options(digits = 20)
.Machine$double.xmin
```

`.Machine$double.eps` returns the smallest positive floating-point number $x$ such that $1 + x! = 1$. In the double-precision binary floating-point format this number is represented by: $$0\ \ 01111001011\ \ 0000000000000000000000000000000000000000000000000000$$
On this computer the value of `.Machine$double.eps` is:

```{r, echo = FALSE}
options(digits = 20)
.Machine$double.eps
```

`.Machine$double.neg.eps` returns the smallest positive floating-point number $x$ such that $1 - x!= 1$. In the double-precision binary floating-point format this number is represented by: $$0\ \ 01111001010\ \ 0000000000000000000000000000000000000000000000000000$$. 
On this computer the value of `.Machine$double.neg.eps` is:

```{r, echo = FALSE}
options(digits = 20)
.Machine$double.neg.eps
```

# Acknowledgment {-}

I would like togive a special thanks to Professor Jun Yan for granting me a deadline extension.

# Reference {-}

[pandoc]: http://pandoc.org/
[pandocManual]: http://pandoc.org/MANUAL.html
[repo]: https://github.com/wenjie2wang/datalab-templates
[taskView]: https://cran.r-project.org/web/views/ReproducibleResearch.html
[shiny.io]: https://www.shinyapps.io/
[wenjie-stat.shinyapps]: https://wwenjie-stat.shinyapps.io/minisplines2
[UCLA]: https://www.math.ucla.edu/~anderson/rw1001/library/base/html/zMachine.html
